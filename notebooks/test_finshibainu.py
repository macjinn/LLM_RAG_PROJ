from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch
import os, time

# ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
MODEL_DIR = "F" 

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (ì €ì¥ëœ ëª¨ë¸ì´ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ìƒëµ)
model_name = "aiqwe/FinShibainu"

if not os.path.exists(MODEL_DIR):
    print(f"ëª¨ë¸ì´ ì €ì¥ëœ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤. `{MODEL_DIR}`ì— ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype="auto"
)
    # ëª¨ë¸ì„ ì§€ì •ëœ í´ë”ì— ì €ì¥
    tokenizer.save_pretrained(MODEL_DIR)
    model.save_pretrained(MODEL_DIR)
    print(f"ëª¨ë¸ì´ `{MODEL_DIR}`ì— ì €ì¥.")
else:
    print("ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
    # 4-bit ì–‘ìí™” ì ìš© (VRAM ì ˆì•½)
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,  # 4-bit ì–‘ìí™” ì ìš©
        bnb_4bit_compute_dtype=torch.float16,  # ê³„ì‚°ì€ FP16 ì‚¬ìš©
        bnb_4bit_use_double_quant=True,  # ì¶”ê°€ ì–‘ìí™” ì ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
    )

    # ëª¨ë¸ ë¡œë“œ (ìë™ìœ¼ë¡œ GPU/CPUì— ìµœì  ë¶„ë°°)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_DIR,
        quantization_config=quantization_config,
        device_map="auto"  # VRAM ë¶€ì¡± ì‹œ ì¼ë¶€ CPUë¡œ ì˜¤í”„ë¡œë“œ
    )
    # ëª¨ë¸ì„ GPU ë˜ëŠ” CPUë¡œ ë¡œë“œ
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ëª¨ë¸ì´ `{device}`ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.")


# ì…ë ¥ ë°ì´í„°
information = """
ì€í–‰: KDBì‚°ì—…ì€í–‰
ìƒí’ˆëª…: KDB Hi ì…ì¶œê¸ˆí†µì¥
ê¸°ë³¸ê¸ˆë¦¬: 1.8
ìµœê³ ê¸ˆë¦¬(ìš°ëŒ€ê¸ˆë¦¬í¬í•¨): 1.8

ì€í–‰: NHë†í˜‘ì€í–‰
ìƒí’ˆëª…: NH1934ìš°ëŒ€í†µì¥
ê¸°ë³¸ê¸ˆë¦¬: 0.1
ìµœê³ ê¸ˆë¦¬(ìš°ëŒ€ê¸ˆë¦¬í¬í•¨): 3.0

ì€í–‰: ì‹ í•œì€í–‰
ìƒí’ˆëª…: ì‹ í•œ ì£¼ê±°ë˜ ë¯¸ë˜ì„¤ê³„í†µì¥
ê¸°ë³¸ê¸ˆë¦¬: 0.1
ìµœê³ ê¸ˆë¦¬(ìš°ëŒ€ê¸ˆë¦¬í¬í•¨): 0.75
"""

# í”„ë¡¬í”„íŠ¸
input_text = f"""
ë‹¹ì‹ ì€ ê¸ˆìœµìƒí’ˆ ì „ë¬¸ê°€ì´ë©° ê¸ˆìœµìƒí’ˆì„ ì¶”ì²œí•˜ëŠ” ìƒë‹´ì› ì—­í• ì…ë‹ˆë‹¤.
ë‹¤ìŒ ê¸ˆìœµ ìƒí’ˆ ë°ì´í„°ì—ì„œ 'ìµœê³ ê¸ˆë¦¬(ìš°ëŒ€ê¸ˆë¦¬í¬í•¨)' ê°’ì´ ê°€ì¥ ë†’ì€ **ë‹¨ í•˜ë‚˜ì˜** ìƒí’ˆì„ ì°¾ì•„ ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ì—¬ ì¶œë ¥í•˜ì„¸ìš”.
íŠ¹íˆ ë‹µë³€ì—ì„œ ìƒí’ˆì¶”ì²œì˜ ì´ìœ ë¥¼ ìƒì„¸í•˜ê²Œ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë•Œ ë°ì´í„° ì •ë³´ì—ì„œ ê·¼ê±°ë¥¼ ë“¤ì–´ ë…¼ë¦¬ì ì´ê³  ìƒì„¸í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.

ê¸ˆìœµ ìƒí’ˆ ë°ì´í„°:
{information}

ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ:
[ì€í–‰ëª…]: [ìƒí’ˆëª…]
ìƒí’ˆì„¤ëª…: 
ìƒí’ˆì¶”ì²œ ì´ìœ : 

"""
print(f"ì…ë ¥:\n{input_text}\n")


# ì…ë ¥ì„ í† í°í™”
device = "cuda" if torch.cuda.is_available() else "cpu"
inputs = tokenizer(input_text, return_tensors="pt").to(device)

start_time = time.time()

# ë‹µë³€ ìƒì„±
print("ëª¨ë¸ì´ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘...\n")
max_length = 800 # ìƒì„± í† í° ìˆ˜
with torch.no_grad():
    output = model.generate(**inputs, max_length=max_length)


# ê²°ê³¼ ë””ì½”ë”©
response_text = tokenizer.decode(output[0], skip_special_tokens=True)

if "ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ:" in response_text:
    response_text = response_text.split("ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ:")[-1].strip()
print(f"ğŸ’¬ ëª¨ë¸ ì‘ë‹µ: {response_text}\n")

end_time = time.time()
print(f"ìƒì„±í•œ í† í° ìˆ˜: {len(output[0])}")
print(f"ëª¨ë¸ ì‘ë‹µ ì†Œìš” ì‹œê°„: {end_time - start_time:.3f}")



