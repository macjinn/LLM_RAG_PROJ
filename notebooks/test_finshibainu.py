from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch
import os, time



# ----------------------------  RTX3080 10gb vram ê¸°ì¤€  ----------------------------  

# # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
# MODEL_DIR = "/home/inseong/LLM_RAG_PROJ/models/FinShibainu_4bit" 

# # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (ì €ì¥ëœ ëª¨ë¸ì´ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ìƒëµ)
# model_name = "aiqwe/FinShibainu"

# if not os.path.exists(MODEL_DIR):
#     print(f"ëª¨ë¸ì´ ì €ì¥ëœ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤. `{MODEL_DIR}`ì— ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...")
#     tokenizer = AutoTokenizer.from_pretrained(model_name)
#     model = AutoModelForCausalLM.from_pretrained(
#     model_name,
#     device_map="auto",
#     torch_dtype="auto"
# )
#     # ëª¨ë¸ì„ ì§€ì •ëœ í´ë”ì— ì €ì¥
#     tokenizer.save_pretrained(MODEL_DIR)
#     model.save_pretrained(MODEL_DIR)
#     print(f"ëª¨ë¸ì´ `{MODEL_DIR}`ì— ì €ì¥.")
# else:
#     print("ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ì¤‘...")
#     tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
#     # 4-bit ì–‘ìí™” ì ìš© (VRAM ì ˆì•½)
#     quantization_config = BitsAndBytesConfig(
#         load_in_4bit=True,  # 4-bit ì–‘ìí™” ì ìš©
#         bnb_4bit_compute_dtype=torch.float16,  # ê³„ì‚°ì€ FP16 ì‚¬ìš©
#         bnb_4bit_use_double_quant=True,  # ì¶”ê°€ ì–‘ìí™” ì ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
#     )

#     # ëª¨ë¸ ë¡œë“œ (ìë™ìœ¼ë¡œ GPU/CPUì— ìµœì  ë¶„ë°°)
#     model = AutoModelForCausalLM.from_pretrained(
#         MODEL_DIR,
#         quantization_config=quantization_config,
#         device_map="auto"  # VRAM ë¶€ì¡± ì‹œ ì¼ë¶€ CPUë¡œ ì˜¤í”„ë¡œë“œ
#     )
#     # ëª¨ë¸ì„ GPU ë˜ëŠ” CPUë¡œ ë¡œë“œ
#     device = "cuda" if torch.cuda.is_available() else "cpu"
#     print(f"ëª¨ë¸ì´ `{device}`ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.")


# ----------------------------  RTX3090 24gb vram ê¸°ì¤€  ----------------------------  


# ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì • (ì–‘ìí™”ëœ ë²„ì „ê³¼ êµ¬ë¶„)
MODEL_DIR = "/home/inseong/LLM_RAG_PROJ/models/FinShibainu_full"
model_name = "aiqwe/FinShibainu"

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

if not os.path.exists(MODEL_DIR):
    print(f"ëª¨ë¸ì´ `{MODEL_DIR}`ì— ì—†ìœ¼ë¯€ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        device_map="auto"
    )
    # ëª¨ë¸ ì €ì¥
    tokenizer.save_pretrained(MODEL_DIR)
    model.save_pretrained(MODEL_DIR)
    print(f"ëª¨ë¸ì´ `{MODEL_DIR}`ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
else:
    print("ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_DIR,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        device_map="auto"
    )
    print(f"ëª¨ë¸ì´ `{device}`ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.")


# --------------------------------------------------------------------------------

# ì…ë ¥ ë°ì´í„°
information = """
ì€í–‰: KDBì‚°ì—…ì€í–‰
ìƒí’ˆëª…: KDB Hi ì…ì¶œê¸ˆí†µì¥
ê¸°ë³¸ê¸ˆë¦¬: 1.8
ìµœê³ ê¸ˆë¦¬(ìš°ëŒ€ê¸ˆë¦¬í¬í•¨): 1.8

ì€í–‰: NHë†í˜‘ì€í–‰
ìƒí’ˆëª…: NH1934ìš°ëŒ€í†µì¥
ê¸°ë³¸ê¸ˆë¦¬: 0.1
ìµœê³ ê¸ˆë¦¬(ìš°ëŒ€ê¸ˆë¦¬í¬í•¨): 3.0

ì€í–‰: ì‹ í•œì€í–‰
ìƒí’ˆëª…: ì‹ í•œ ì£¼ê±°ë˜ ë¯¸ë˜ì„¤ê³„í†µì¥
ê¸°ë³¸ê¸ˆë¦¬: 0.1
ìµœê³ ê¸ˆë¦¬(ìš°ëŒ€ê¸ˆë¦¬í¬í•¨): 0.75

ì€í–‰: SCì œì¼ì€í–‰
ìƒí’ˆëª…: ë‚´ì›”ê¸‰í†µì¥
ê¸°ë³¸ê¸ˆë¦¬: 0.6
ìµœê³ ê¸ˆë¦¬(ìš°ëŒ€ê¸ˆë¦¬í¬í•¨): 3.1
ì´ìì§€ê¸‰ë°©ì‹: ì›”ì§€ê¸‰
"""

# í”„ë¡¬í”„íŠ¸
# input_text = f"""
# ë‹¹ì‹ ì€ ê¸ˆìœµìƒí’ˆ ì „ë¬¸ê°€ì´ë©° ê¸ˆìœµìƒí’ˆì„ ì¶”ì²œí•˜ëŠ” ìƒë‹´ì› ì—­í• ì…ë‹ˆë‹¤.
# ë‹¤ìŒ ê¸ˆìœµ ìƒí’ˆ ë°ì´í„°ì—ì„œ 'ìµœê³ ê¸ˆë¦¬(ìš°ëŒ€ê¸ˆë¦¬í¬í•¨)' ê°’ì´ ê°€ì¥ ë†’ì€ **ë‹¨ í•˜ë‚˜ì˜** ìƒí’ˆì„ ì°¾ì•„ ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ì—¬ ì¶œë ¥í•˜ì„¸ìš”.
# íŠ¹íˆ ë‹µë³€ì—ì„œ ìƒí’ˆì¶”ì²œì˜ ì´ìœ ë¥¼ ìƒì„¸í•˜ê²Œ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë•Œ ë°ì´í„° ì •ë³´ì—ì„œ ê·¼ê±°ë¥¼ ë“¤ì–´ ë…¼ë¦¬ì ì´ê³  ìƒì„¸í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.

# ê¸ˆìœµ ìƒí’ˆ ë°ì´í„°:
# {information}

# ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ:
# [ì€í–‰ëª…]: [ìƒí’ˆëª…]
# ìƒí’ˆì„¤ëª…: 
# ìƒí’ˆì¶”ì²œ ì´ìœ : 

# """
# print(f"ì…ë ¥:\n{input_text}\n")



# input_text = f"""
#     You are a financial product expert and consultant who always responds in Korean. Your task is to analyze the given financial product data and recommend exactly one product that has the highest "ìµœê³ ê¸ˆë¦¬(ìš°ëŒ€ê¸ˆë¦¬í¬í•¨)" (highest interest rate including preferential rates).

#     Please follow these instructions carefully:
#     1. Use the provided data only. Do not add any information that is not present in the data.
#     2. If you do not know the answer or if the data does not contain sufficient information, simply respond with "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤" (I don't know). Do not fabricate an answer.
#     3. Clearly extract and present the key details: Bank Name, Product Name, Basic Interest Rate, Highest Interest Rate (including preferential rate), and any relevant conditions or restrictions.
#     4. Provide a detailed recommendation reason based solely on the data, explaining why this product is the best choice.
#     5. Format your answer exactly as shown in the output format below.

#     Financial Product Data:
#     {information}

#     Output Format Example:
#     [ì€í–‰ëª…]: [ìƒí’ˆëª…]
#     ê¸°ë³¸ê¸ˆë¦¬: 
#     ìµœê³ ê¸ˆë¦¬(ìš°ëŒ€ê¸ˆë¦¬í¬í•¨): 
#     ê°€ì…ì¡°ê±´/ì œí•œ: 
#     ì¶”ì²œ ì‚¬ìœ : 

#     Answer in Korean.
#     """

input_text = f"""

    You are a financial product expert and consultant who always responds in Korean.
    Your task is to analyze the user query and the given financial product data to recommend the most suitable financial product.

    ## User Query:
    ì²­ë…„ì„ ìœ„í•œ ìš°ëŒ€ê¸ˆë¦¬ê°€ ì ìš©ëœ ì˜ˆê¸ˆì´ ìˆì„ê¹Œ?

    ## Financial Product Data:
    ì¹´í…Œê³ ë¦¬: ìƒí’ˆ ê°œìš”
ìƒí’ˆ íŠ¹ì§•: ë³‘ì—­ì˜ë¬´ìˆ˜í–‰ìë“¤ì˜ ì „ì—­ í›„ ëª©ëˆ ë§ˆë ¨ì„ ìœ„í•œ ê³ ê¸ˆë¦¬ ììœ ì ë¦½ì‹ ì •ê¸°ì ê¸ˆ ìƒí’ˆ
ì¶œì‹œì¼: 2018ë…„ 8ì›” 29ì¼
ì´ì í˜œíƒ: ë§Œê¸° í•´ì§€ ì‹œ ì´ìì†Œë“ ë¹„ê³¼ì„¸ ë° 1% ì´ìì§€ì›ê¸ˆ ì§€ê¸‰
ë§¤ì¹­ ì§€ì›ê¸ˆ: ì •ë¶€ì—ì„œ ì§€ê¸‰, ë³‘ì—­ë²• ì‹œí–‰ë ¹ ì œ158ì¡°ì˜2ì— ë”°ë¼ ê²°ì •

ì€í–‰: BNKë¶€ì‚°ì€í–‰
ìƒí’ˆëª…: ë”(The) íŠ¹íŒ ì •ê¸°ì˜ˆê¸ˆ
ê¸°ë³¸ê¸ˆë¦¬(ë‹¨ë¦¬ì´ì %): 2.75
ìµœê³ ê¸ˆë¦¬(ìš°ëŒ€ê¸ˆë¦¬í¬í•¨, ë‹¨ë¦¬ì´ì %): 3.2
ì€í–‰ ìµœì¢…ì œê³µì¼: 2025-01-20
ë§Œê¸° í›„ ê¸ˆë¦¬: - ë§Œê¸°í›„1ë…„ë‚´: ê°€ì…ê¸°ê°„ë³„ ì¼ë°˜ì •ê¸°ì˜ˆê¸ˆì´ìœ¨ x 50%,
- ë§Œê¸°í›„1ë…„ì´ˆê³¼:ê°€ì…ê¸°ê°„ë³„ ì¼ë°˜ì •ê¸°ì˜ˆê¸ˆì´ìœ¨ x 20%
ê°€ì…ë°©ë²•: ì¸í„°ë„·ë±…í‚¹,ìŠ¤ë§ˆíŠ¸ë±…í‚¹
ìš°ëŒ€ì¡°ê±´: * ìš°ëŒ€ì´ìœ¨ (ìµœëŒ€ 0.45%p)
ê°€. ëª¨ë°”ì¼ë±…í‚¹ ê¸ˆìœµì •ë³´ ë° í˜œíƒì•Œë¦¼ ë™ì˜ ìš°ëŒ€ì´ìœ¨ : 0.10%p
ë‚˜. ì´ë²¤íŠ¸ ìš°ëŒ€ì´ìœ¨ : ìµœëŒ€ 0.35%p 
1) ë”(The) íŠ¹íŒ ì •ê¸°ì˜ˆê¸ˆ ì‹ ê·œê³ ê° ìš°ëŒ€ì´ìœ¨ : 0.20%p
2) íŠ¹íŒ ìš°ëŒ€ì´ìœ¨ : 0.15%p
ê°€ì… ì œí•œì¡°ê±´: ì œí•œì—†ìŒ
ê°€ì…ëŒ€ìƒ: ì‹¤ëª…ì˜ ê°œì¸
ê¸°íƒ€ ìœ ì˜ì‚¬í•­: 1. ê°€ì…ê¸ˆì•¡ : 1ë°±ë§Œì› ì´ìƒ ì œí•œì—†ìŒ (ì›ë‹¨ìœ„)
2. ê°€ì…ê¸°ê°„ : 1ê°œì›”, 3ê°œì›”, 6ê°œì›”, 1ë…„, 2ë…„, 3ë…„
3. ì´ìì§€ê¸‰ë°©ì‹ : ë§Œê¸°ì¼ì‹œì§€ê¸‰ì‹
ìµœê³ í•œë„: ì •ë³´ ì—†ìŒ
ì „ì›”ì·¨ê¸‰í‰ê· ê¸ˆë¦¬(ë§Œê¸° 12ê°œì›” ê¸°ì¤€): 3.1

ì€í–‰: Shìˆ˜í˜‘ì€í–‰
ìƒí’ˆëª…: Shí‰ìƒì£¼ê±°ë˜ìš°ëŒ€ì˜ˆê¸ˆ
(ë§Œê¸°ì¼ì‹œì§€ê¸‰ì‹)
ê¸°ë³¸ê¸ˆë¦¬(ë‹¨ë¦¬ì´ì %): 2.4
ìµœê³ ê¸ˆë¦¬(ìš°ëŒ€ê¸ˆë¦¬í¬í•¨, ë‹¨ë¦¬ì´ì %): 2.8
ì€í–‰ ìµœì¢…ì œê³µì¼: 2025-01-20
ë§Œê¸° í›„ ê¸ˆë¦¬: * ë§Œê¸°í›„ 1ë…„ ì´ë‚´
 - ë§Œê¸°ë‹¹ì‹œ ì¼ë°˜ì •ê¸°ì˜ˆê¸ˆ(ì›”ì´ìì§€ê¸‰ì‹) ê³„ì•½ê¸°ê°„ë³„ ê¸°ë³¸ê¸ˆë¦¬ 1/2
* ë§Œê¸°í›„ 1ë…„ ì´ˆê³¼
 - ë§Œê¸°ë‹¹ì‹œ ë³´í†µì˜ˆê¸ˆ ê¸°ë³¸ê¸ˆë¦¬
ê°€ì…ë°©ë²•: ì˜ì—…ì 
ìš°ëŒ€ì¡°ê±´: * ìµœëŒ€ìš°ëŒ€ê¸ˆë¦¬ : 0.4%
1. ê±°ë˜ê³ ê°ìš°ëŒ€ê¸ˆë¦¬ : ìµœëŒ€0.1% (ì‹ ê·œì‹œ) 
 -ìµœì´ˆì˜ˆì ê¸ˆê³ ê°/ì¬ì˜ˆì¹˜/ì¥ê¸°ê±°ë˜ ê° 0.05% 
2. ê±°ë˜ì‹¤ì ìš°ëŒ€ê¸ˆë¦¬ : ìµœëŒ€0.3% (ë§Œê¸°ì‹œ)
 -ê¸‰ì—¬,ì—°ê¸ˆì´ì²´ë“±/ìˆ˜í˜‘ì¹´ë“œê²°ì œ/ê³µê³¼ê¸ˆì´ì²´ë“± ê°0.1%
â€»ë‹¨ìœ„:ì—°%p
ê°€ì… ì œí•œì¡°ê±´: ì œí•œì—†ìŒ
ê°€ì…ëŒ€ìƒ: ì‹¤ëª…ì˜ ê°œì¸
ê¸°íƒ€ ìœ ì˜ì‚¬í•­: - 1ì¸ 1ê³„ì¢Œ
- ìµœì € 100ë§Œì› ì´ìƒ
ìµœê³ í•œë„: ì •ë³´ ì—†ìŒ
ì „ì›”ì·¨ê¸‰í‰ê· ê¸ˆë¦¬(ë§Œê¸° 12ê°œì›” ê¸°ì¤€): 2.4

    ## Instructions:
    1. Use only the provided data. Do not add any information that is not present in the data.
    2. If you do not know the answer or the data is insufficient, respond with "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤".
    3. Clearly extract and present the following details:
       - Bank Name
       - Product Name
       - Basic Interest Rate
       - Highest Interest Rate (including preferential rate)
       - Conditions/Restrictions
       - Recommendation Reason

    Please provide your final answer after the tag [USER_ANSWER]: 
    
    """

# ì…ë ¥ì„ í† í°í™”
device = "cuda" if torch.cuda.is_available() else "cpu"
inputs = tokenizer(input_text, return_tensors="pt").to(device)

start_time = time.time()

# ë‹µë³€ ìƒì„±
print("ëª¨ë¸ì´ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘...\n")
max_length = 2000 # ìƒì„± í† í° ìˆ˜
with torch.no_grad():
    output = model.generate(**inputs, max_length=max_length)


# ê²°ê³¼ ë””ì½”ë”©
response_text = tokenizer.decode(output[0], skip_special_tokens=True)

if "Answer in Korean." in response_text:
    response_text = response_text.split("Answer in Korean.")[-1].strip()
print(f"ğŸ’¬ ëª¨ë¸ ì‘ë‹µ: {response_text}\n")

end_time = time.time()
print(f"ìƒì„±í•œ í† í° ìˆ˜: {len(output[0])}")
print(f"ëª¨ë¸ ì‘ë‹µ ì†Œìš” ì‹œê°„: {end_time - start_time:.3f}")



