from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch
import os, time

# ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
MODEL_DIR = "/home/inseong/LLM_RAG_PROJ/models/FinShibainu_4bit" 

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (ì €ì¥ëœ ëª¨ë¸ì´ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ìƒëµ)
model_name = "aiqwe/FinShibainu"

if not os.path.exists(MODEL_DIR):
    print(f"ëª¨ë¸ì´ ì €ì¥ëœ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤. `{MODEL_DIR}`ì— ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype="auto"
)
    # ëª¨ë¸ì„ ì§€ì •ëœ í´ë”ì— ì €ì¥
    tokenizer.save_pretrained(MODEL_DIR)
    model.save_pretrained(MODEL_DIR)
    print(f"ëª¨ë¸ì´ `{MODEL_DIR}`ì— ì €ì¥.")
else:
    print("ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
    # 4-bit ì–‘ìí™” ì ìš© (VRAM ì ˆì•½)
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,  # 4-bit ì–‘ìí™” ì ìš©
        bnb_4bit_compute_dtype=torch.float16,  # ê³„ì‚°ì€ FP16 ì‚¬ìš©
        bnb_4bit_use_double_quant=True,  # ì¶”ê°€ ì–‘ìí™” ì ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
    )

    # ëª¨ë¸ ë¡œë“œ (ìë™ìœ¼ë¡œ GPU/CPUì— ìµœì  ë¶„ë°°)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_DIR,
        quantization_config=quantization_config,
        device_map="auto"  # VRAM ë¶€ì¡± ì‹œ ì¼ë¶€ CPUë¡œ ì˜¤í”„ë¡œë“œ
    )
    # ëª¨ë¸ì„ GPU ë˜ëŠ” CPUë¡œ ë¡œë“œ
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ëª¨ë¸ì´ `{device}`ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.")


# ì…ë ¥ ë°ì´í„°
information = """
ì€í–‰: KDBì‚°ì—…ì€í–‰
ìƒí’ˆëª…: KDB Hi ì…ì¶œê¸ˆí†µì¥
ê¸°ë³¸ê¸ˆë¦¬: 1.8
ìµœê³ ê¸ˆë¦¬(ìš°ëŒ€ê¸ˆë¦¬í¬í•¨): 1.8

ì€í–‰: NHë†í˜‘ì€í–‰
ìƒí’ˆëª…: NH1934ìš°ëŒ€í†µì¥
ê¸°ë³¸ê¸ˆë¦¬: 0.1
ìµœê³ ê¸ˆë¦¬(ìš°ëŒ€ê¸ˆë¦¬í¬í•¨): 3.0

ì€í–‰: ì‹ í•œì€í–‰
ìƒí’ˆëª…: ì‹ í•œ ì£¼ê±°ë˜ ë¯¸ë˜ì„¤ê³„í†µì¥
ê¸°ë³¸ê¸ˆë¦¬: 0.1
ìµœê³ ê¸ˆë¦¬(ìš°ëŒ€ê¸ˆë¦¬í¬í•¨): 0.75

ì€í–‰: SCì œì¼ì€í–‰
ìƒí’ˆëª…: ë‚´ì›”ê¸‰í†µì¥
ê¸°ë³¸ê¸ˆë¦¬: 0.6
ìµœê³ ê¸ˆë¦¬(ìš°ëŒ€ê¸ˆë¦¬í¬í•¨): 3.1
ì´ìì§€ê¸‰ë°©ì‹: ì›”ì§€ê¸‰
"""

# í”„ë¡¬í”„íŠ¸
# input_text = f"""
# ë‹¹ì‹ ì€ ê¸ˆìœµìƒí’ˆ ì „ë¬¸ê°€ì´ë©° ê¸ˆìœµìƒí’ˆì„ ì¶”ì²œí•˜ëŠ” ìƒë‹´ì› ì—­í• ì…ë‹ˆë‹¤.
# ë‹¤ìŒ ê¸ˆìœµ ìƒí’ˆ ë°ì´í„°ì—ì„œ 'ìµœê³ ê¸ˆë¦¬(ìš°ëŒ€ê¸ˆë¦¬í¬í•¨)' ê°’ì´ ê°€ì¥ ë†’ì€ **ë‹¨ í•˜ë‚˜ì˜** ìƒí’ˆì„ ì°¾ì•„ ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ì—¬ ì¶œë ¥í•˜ì„¸ìš”.
# íŠ¹íˆ ë‹µë³€ì—ì„œ ìƒí’ˆì¶”ì²œì˜ ì´ìœ ë¥¼ ìƒì„¸í•˜ê²Œ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë•Œ ë°ì´í„° ì •ë³´ì—ì„œ ê·¼ê±°ë¥¼ ë“¤ì–´ ë…¼ë¦¬ì ì´ê³  ìƒì„¸í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.

# ê¸ˆìœµ ìƒí’ˆ ë°ì´í„°:
# {information}

# ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ:
# [ì€í–‰ëª…]: [ìƒí’ˆëª…]
# ìƒí’ˆì„¤ëª…: 
# ìƒí’ˆì¶”ì²œ ì´ìœ : 

# """
# print(f"ì…ë ¥:\n{input_text}\n")



# input_text = f"""
#     You are a financial product expert and consultant who always responds in Korean. Your task is to analyze the given financial product data and recommend exactly one product that has the highest "ìµœê³ ê¸ˆë¦¬(ìš°ëŒ€ê¸ˆë¦¬í¬í•¨)" (highest interest rate including preferential rates).

#     Please follow these instructions carefully:
#     1. Use the provided data only. Do not add any information that is not present in the data.
#     2. If you do not know the answer or if the data does not contain sufficient information, simply respond with "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤" (I don't know). Do not fabricate an answer.
#     3. Clearly extract and present the key details: Bank Name, Product Name, Basic Interest Rate, Highest Interest Rate (including preferential rate), and any relevant conditions or restrictions.
#     4. Provide a detailed recommendation reason based solely on the data, explaining why this product is the best choice.
#     5. Format your answer exactly as shown in the output format below.

#     Financial Product Data:
#     {information}

#     Output Format Example:
#     [ì€í–‰ëª…]: [ìƒí’ˆëª…]
#     ê¸°ë³¸ê¸ˆë¦¬: 
#     ìµœê³ ê¸ˆë¦¬(ìš°ëŒ€ê¸ˆë¦¬í¬í•¨): 
#     ê°€ì…ì¡°ê±´/ì œí•œ: 
#     ì¶”ì²œ ì‚¬ìœ : 

#     Answer in Korean.
#     """

input_text = f"""
    ëŒ€í•œë¯¼êµ­ ê²½ì œìƒí™©ì„ ì¢…í•©ì ìœ¼ë¡œ ì •ë¦¬í•˜ê³  ì•ìœ¼ë¡œì˜ ê²½ì œ ë™í–¥ì„ ë³´ê³ ì„œì˜ í˜•íƒœë¡œ ì‘ì„±í•´ì¤˜.

    Answer in Korean.
    """

# ì…ë ¥ì„ í† í°í™”
device = "cuda" if torch.cuda.is_available() else "cpu"
inputs = tokenizer(input_text, return_tensors="pt").to(device)

start_time = time.time()

# ë‹µë³€ ìƒì„±
print("ëª¨ë¸ì´ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘...\n")
max_length = 1000 # ìƒì„± í† í° ìˆ˜
with torch.no_grad():
    output = model.generate(**inputs, max_length=max_length)


# ê²°ê³¼ ë””ì½”ë”©
response_text = tokenizer.decode(output[0], skip_special_tokens=True)

if "Answer in Korean." in response_text:
    response_text = response_text.split("Answer in Korean.")[-1].strip()
print(f"ğŸ’¬ ëª¨ë¸ ì‘ë‹µ: {response_text}\n")

end_time = time.time()
print(f"ìƒì„±í•œ í† í° ìˆ˜: {len(output[0])}")
print(f"ëª¨ë¸ ì‘ë‹µ ì†Œìš” ì‹œê°„: {end_time - start_time:.3f}")



