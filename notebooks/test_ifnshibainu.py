from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os

# ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
MODEL_DIR = "/home/s2019105385/2025_1/LLM_RAG_PROJ/models/FinShibainu" 

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (ì €ì¥ëœ ëª¨ë¸ì´ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ìƒëµ)
model_name = "aiqwe/FinShibainu"

if not os.path.exists(MODEL_DIR):
    print(f"ëª¨ë¸ì´ ì €ì¥ëœ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤. `{MODEL_DIR}`ì— ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype="auto"
)
    # ëª¨ë¸ì„ ì§€ì •ëœ í´ë”ì— ì €ì¥
    tokenizer.save_pretrained(MODEL_DIR)
    model.save_pretrained(MODEL_DIR)
    print(f"ëª¨ë¸ì´ `{MODEL_DIR}`ì— ì €ì¥.")
else:
    print(f"ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ.")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
    model = AutoModelForCausalLM.from_pretrained(MODEL_DIR)

# ëª¨ë¸ì„ GPU ë˜ëŠ” CPUë¡œ ë¡œë“œ
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
print(f"ëª¨ë¸ì´ `{device}`ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

# ì…ë ¥ ì˜ˆì‹œ
input_text = "ê¸ˆë¦¬ê°€ ë†’ì€ ì ê¸ˆ ìƒí’ˆ ì¶”ì²œí•´ì¤˜"

# ì…ë ¥ì„ í† í°í™”
device = "cuda" if torch.cuda.is_available() else "cpu"
inputs = tokenizer(input_text, return_tensors="pt").to(device)


# ëª¨ë¸ ì˜ˆì¸¡
with torch.no_grad():
    output = model.generate(**inputs, max_length=100)

# ê²°ê³¼ ë””ì½”ë”©
response = tokenizer.decode(output[0], skip_special_tokens=True)
print(f"ğŸ’¬ ëª¨ë¸ ì‘ë‹µ: {response}")
