from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch
import os, time

# ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
MODEL_DIR = "/home/inseong/LLM_RAG_PROJ/models/FinShibainu_4bit" 

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (ì €ì¥ëœ ëª¨ë¸ì´ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ìƒëµ)
model_name = "aiqwe/FinShibainu"

if not os.path.exists(MODEL_DIR):
    print(f"ëª¨ë¸ì´ ì €ì¥ëœ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤. `{MODEL_DIR}`ì— ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype="auto"
)
    # ëª¨ë¸ì„ ì§€ì •ëœ í´ë”ì— ì €ì¥
    tokenizer.save_pretrained(MODEL_DIR)
    model.save_pretrained(MODEL_DIR)
    print(f"ëª¨ë¸ì´ `{MODEL_DIR}`ì— ì €ì¥.")
else:
    print("ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
    # 4-bit ì–‘ìí™” ì ìš© (VRAM ì ˆì•½)
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,  # 4-bit ì–‘ìí™” ì ìš©
        bnb_4bit_compute_dtype=torch.float16,  # ê³„ì‚°ì€ FP16 ì‚¬ìš©
        bnb_4bit_use_double_quant=True,  # ì¶”ê°€ ì–‘ìí™” ì ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
    )

    # ëª¨ë¸ ë¡œë“œ (ìë™ìœ¼ë¡œ GPU/CPUì— ìµœì  ë¶„ë°°)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_DIR,
        quantization_config=quantization_config,
        device_map="auto"  # VRAM ë¶€ì¡± ì‹œ ì¼ë¶€ CPUë¡œ ì˜¤í”„ë¡œë“œ
    )
    # ëª¨ë¸ì„ GPU ë˜ëŠ” CPUë¡œ ë¡œë“œ
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ëª¨ë¸ì´ `{device}`ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.")


# ì…ë ¥
information = 'ì€í–‰: KDBì‚°ì—…ì€í–‰\nìƒí’ˆëª…: KDB Hi ì…ì¶œê¸ˆí†µì¥\nê¸°ë³¸ê¸ˆë¦¬: 1.8\nìµœê³ ê¸ˆë¦¬(ìš°ëŒ€ê¸ˆë¦¬í¬í•¨): 1.8, ì€í–‰: NHë†í˜‘ì€í–‰\nìƒí’ˆëª…: NH1934ìš°ëŒ€í†µì¥\nê¸°ë³¸ê¸ˆë¦¬: 0.1\nìµœê³ ê¸ˆë¦¬(ìš°ëŒ€ê¸ˆë¦¬í¬í•¨): 3.0\nì´ìì§€ê¸‰ë°©ì‹: ë¶„ê¸°ì§€ê¸‰'
input_text = f"ë‹¤ìŒì˜ ê¸ˆìœµì‚¬í’ˆ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ê¸ˆë¦¬ê°€ ë†’ì€ ì ê¸ˆ ìƒí’ˆ 1ê°œë§Œ ì¶”ì²œí•´ì¤˜ \n ì •ë³´ \n {information}"
print(f"ì…ë ¥:{input_text}\n")

# ì…ë ¥ì„ í† í°í™”
device = "cuda" if torch.cuda.is_available() else "cpu"
inputs = tokenizer(input_text, return_tensors="pt").to(device)

start_time = time.time()

# ë‹µë³€ ìƒì„±
print("ëª¨ë¸ì´ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘...\n")
max_length = 700 # ìƒì„± í† í° ìˆ˜
with torch.no_grad():
    output = model.generate(**inputs, max_length=max_length)

# ê²°ê³¼ ë””ì½”ë”©
response = tokenizer.decode(output[0], skip_special_tokens=True)
print(f"ğŸ’¬ ëª¨ë¸ ì‘ë‹µ: {response}\n")

end_time = time.time()
print(f"ìƒì„±í•œ í† í° ìˆ˜: {len(response)}")
print(f"ëª¨ë¸ ì‘ë‹µ ì†Œìš” ì‹œê°„: {end_time - start_time:.3f}")



